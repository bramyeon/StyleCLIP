{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# StyleCLIP+: Towards an Extended and User-Friendly StyleCLIP, <b>Gradio Demo 2</b>\n","Fall 2023 CS470 Introduction to Artificial Intelligence Project 2 (Implementation), Team 3  \n","Myeongseok Kwon<sup>1</sup>, Junhak Ha<sup>2</sup>, Dongwan Hong<sup>3</sup>, Kyeongmin Lee<sup>4</sup>, Bryan Nathanael Wijaya<sup>5*</sup>  \n","<small><sup>1</sup>20170042, <sup>2</sup>20190683, <sup>3</sup>20190696, <sup>4</sup>20200429, <sup>5</sup>20200735, <sup>*</sup>Team Leader</small>\n","\n","## Ablation Study of MSE Loss Usage"],"metadata":{"id":"ofF4wX1ojGWh"}},{"cell_type":"code","source":["#@title Setting up Gradio for demo\n","!pip install kaleido cohere openai tiktoken gradio -q\n","import gradio as gr"],"metadata":{"id":"RsbaFs_SGYaC","executionInfo":{"status":"ok","timestamp":1701540586478,"user_tz":-540,"elapsed":39109,"user":{"displayName":"Bryan Nathanael Wijaya","userId":"03815891222670931187"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"320f9a72-dc2c-482e-c334-4eeb13568a8b","cellView":"form"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["#@title Environment setup\n","import time, os\n","import torch\n","from torchvision.utils import make_grid\n","from torchvision.transforms import ToPILImage\n","if not os.path.exists('./StyleCLIP'):\n","    !git clone https://github.com/bramyeon/StyleCLIP.git\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","from tqdm import tqdm\n","\n","# Development environment setup for latent optimization\n","os.chdir(f'./StyleCLIP')\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","# downloads StyleGAN's weights and facial recognition network weights\n","from StyleCLIP.utils import google_drive_paths\n","ckpt = 'stylegan2-ffhq-config-f.pt'\n","ckpt_id = google_drive_paths[ckpt].split('=')[-1]\n","ids = [ckpt_id, '1N0MZSqPRJpLfP4mFQCS14ikrVSe8vQlL']\n","for file_id in ids:\n","    downloaded = drive.CreateFile({'id':file_id})\n","    downloaded.FetchMetadata(fetch_all=True)\n","    downloaded.GetContentFile(downloaded.metadata['title'])\n","from StyleCLIP.optimization.run_optimization import main\n","from argparse import Namespace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701540684930,"user_tz":-540,"elapsed":98456,"user":{"displayName":"Bryan Nathanael Wijaya","userId":"03815891222670931187"}},"outputId":"8d3ccf6f-5c99-49cd-8b12-36e90cc29d6b","id":"wch39m58NueC","cellView":"form"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'StyleCLIP'...\n","remote: Enumerating objects: 925, done.\u001b[K\n","remote: Counting objects: 100% (307/307), done.\u001b[K\n","remote: Compressing objects: 100% (159/159), done.\u001b[K\n","remote: Total 925 (delta 186), reused 211 (delta 142), pack-reused 618\u001b[K\n","Receiving objects: 100% (925/925), 200.06 MiB | 21.21 MiB/s, done.\n","Resolving deltas: 100% (331/331), done.\n","Collecting ftfy\n","  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m911.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.12)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.1.3\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-d2zyj0sp\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-d2zyj0sp\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.8.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=c68b71e7b91f302ebe6066e265dba0c58dfe85f0856a148c19949aba3dc25f2e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-n8v0pu30/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n"]}]},{"cell_type":"code","source":["#@title Demo 2 Gradio\n","\n","def demo2_fn(seed, text_prompt, optimization_steps, l2_lambda, id_lambda, mse_lambda):\n","    model = \"StyleGAN2, FFHQ, human face, stylegan2-ffhq-config-f.pt\"\n","    architecture, dataset, description, ckpt = model.split(', ')\n","\n","    # setting up the arguments for Latent Optimization\n","    experiment_type = 'edit'\n","    latent_path = None\n","    stylespace = False\n","    create_video = False\n","\n","    args = {\n","        \"description\": text_prompt,\n","        \"ckpt\": ckpt,\n","        \"stylegan_size\": 1024,\n","        \"lr_rampup\": 0.05,\n","        \"lr\": 0.1,\n","        \"step\": optimization_steps,\n","        \"mode\": experiment_type,\n","        \"l2_lambda\": l2_lambda,\n","        \"id_lambda\": id_lambda,\n","        \"mse_lambda\": 0,\n","        'work_in_stylespace': stylespace,\n","        \"latent_path\": latent_path,\n","        \"truncation\": 0.7,\n","        \"save_intermediate_image_every\": 1 if create_video else 20,\n","        \"results_dir\": \"results\",\n","        \"ir_se50_weights\": \"model_ir_se50.pth\"\n","    }\n","\n","    torch.manual_seed(seed)\n","    result = main(Namespace(**args))\n","    result_image = ToPILImage()(make_grid(result.detach().cpu(), normalize=True, scale_each=True, padding=0)) #range=(-1, 1)\n","    h, w = result_image.size\n","    result_image.resize((h // 2, w // 2))\n","\n","    # with MSE loss\n","    args['mse_lambda'] = mse_lambda\n","    torch.manual_seed(seed)\n","    result_mse = main(Namespace(**args))\n","    result_image_mse = ToPILImage()(make_grid(result_mse.detach().cpu(), normalize=True, scale_each=True, padding=0)) #range=(-1, 1)\n","    h, w = result_image_mse.size\n","    result_image_mse.resize((h // 2, w // 2))\n","\n","    return result_image, result_image_mse\n","\n","demo2 = gr.Interface(\n","    fn=demo2_fn, title=\"CS470 Team 3 MSE Loss Demo\",\n","    inputs=[gr.Slider(0, 1000, step=1, value=534, label=\"Seed for original FACE image\"), \\\n","            gr.Textbox(lines=1, placeholder=\"e.g., an angry face\", label=\"Target prompt\"), \\\n","            gr.Slider(0, 500, step=1, value=40, label=\"Optimization steps\"), \\\n","            gr.Slider(0, 0.1, step=0.001, value=0.008, label=\"L2-loss strength\"), \\\n","            gr.Slider(0, 0.1, step=0.001, value=0.005, label=\"ID-loss strength\"), \\\n","            gr.Slider(0, 1, step=0.005, value=0.5, label=\"MSE-loss strength (for the second case)\")],\n","    outputs=[gr.Image(label=\"Original vs. target image (w/o MSE loss)\"), \\\n","             gr.Image(label=\"Original vs. target image (with MSE loss)\")])\n","\n","if __name__ == \"__main__\":\n","    demo2.launch(show_api=False, share=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":611},"outputId":"5090c437-ea0b-46f2-f5d9-f5074b397e6f","executionInfo":{"status":"ok","timestamp":1701540861892,"user_tz":-540,"elapsed":2698,"user":{"displayName":"Bryan Nathanael Wijaya","userId":"03815891222670931187"}},"id":"f6e50Sh5NueC"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://21a4d2d204ad201268.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://21a4d2d204ad201268.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"drYzMnEtvG3V"},"execution_count":null,"outputs":[]}]}