{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# StyleCLIP+: Towards an Extended and User-Friendly StyleCLIP\n",
        "Fall 2023 CS470 Introduction to Artificial Intelligence Project 2 (Implementation), Team 3  \n",
        "Myeongseok Kwon<sup>1</sup>, Junhak Ha<sup>2</sup>, Dongwan Hong<sup>3</sup>, Kyeongmin Lee<sup>4</sup>, Bryan Nathanael Wijaya<sup>5*</sup>  \n",
        "<small><sup>1</sup>20170042, <sup>2</sup>20190683, <sup>3</sup>20190696, <sup>4</sup>20200429, <sup>5</sup>20200735, <sup>*</sup>Team Leader</small>"
      ],
      "metadata": {
        "id": "U6DPGQRamouV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pNWhaK8NdpMv"
      },
      "outputs": [],
      "source": [
        "import time, os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setting up input image, text prompt, and the StyleGAN model\n",
        "\n",
        "#@markdown Specify the `text_prompt` you want to use to manipulate the image.\n",
        "text_prompt = \"a salty fish\" #@param {type:\"string\"}\n",
        "#@markdown Choose your pretrained StyleGAN `model`.\n",
        "model = \"human_face\" #@param [\"human_face\", \"car\", \"painting\"]\n",
        "use_llm = False"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vbxKygir4irR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Refining text prompt with LLM"
      ],
      "metadata": {
        "id": "c79bbym74jQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Downloading Llama 2 Chat model checkpoint with 7B parameters (skip if already exists)\n",
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_1.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "JuAKExrjg-qs",
        "outputId": "59075d08-cc85-478f-bc63-735b3dadf83e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-13 18:03:08--  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_1.bin\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.7.38, 13.35.7.81, 13.35.7.57, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.7.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/30/e3/30e3aca7233f7337633262ff6d59dd98559ecd8982e7419b39752c8d0daae1ca/b379a454d32593089f663914c5a4687d95fd37e3bcddf826f9fb107598a8ff50?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.ggmlv3.q4_1.bin%3B+filename%3D%22llama-2-7b-chat.ggmlv3.q4_1.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1700157789&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMDE1Nzc4OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMC9lMy8zMGUzYWNhNzIzM2Y3MzM3NjMzMjYyZmY2ZDU5ZGQ5ODU1OWVjZDg5ODJlNzQxOWIzOTc1MmM4ZDBkYWFlMWNhL2IzNzlhNDU0ZDMyNTkzMDg5ZjY2MzkxNGM1YTQ2ODdkOTVmZDM3ZTNiY2RkZjgyNmY5ZmIxMDc1OThhOGZmNTA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=oDnf0ZhxPraFL4ubDIIAMuohcESX4JKZQfat7Pi3FbzDbkeJC4ZdlAgq0dBG91IfOBoj6glIj7ktdqTbkPUSKiyx33nmeIr9je1O0bSH887gqQRyoJ03rUJNhWPe-%7ExdljrOqsuh51MzdBws8Sr1SqjDPpeZ1rBaf1PF50fRgveiMZw%7Ezdnu756sn4xugy9QPqw5YlDd3IqX3J9EMvaSqEgM0bkzeak3KTofHGEGsR9GFT9JaUxyfN10XwgVjwl2DEijd49iFNXWrwB1t-NgJia7x9wKE1tQsHUfqvE8m-WOl5lwTBKHQ6TMB1eGoFYqLqJ7rBxAjZynqHbyi6mHwA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-11-13 18:03:09--  https://cdn-lfs.huggingface.co/repos/30/e3/30e3aca7233f7337633262ff6d59dd98559ecd8982e7419b39752c8d0daae1ca/b379a454d32593089f663914c5a4687d95fd37e3bcddf826f9fb107598a8ff50?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.ggmlv3.q4_1.bin%3B+filename%3D%22llama-2-7b-chat.ggmlv3.q4_1.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1700157789&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMDE1Nzc4OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMC9lMy8zMGUzYWNhNzIzM2Y3MzM3NjMzMjYyZmY2ZDU5ZGQ5ODU1OWVjZDg5ODJlNzQxOWIzOTc1MmM4ZDBkYWFlMWNhL2IzNzlhNDU0ZDMyNTkzMDg5ZjY2MzkxNGM1YTQ2ODdkOTVmZDM3ZTNiY2RkZjgyNmY5ZmIxMDc1OThhOGZmNTA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=oDnf0ZhxPraFL4ubDIIAMuohcESX4JKZQfat7Pi3FbzDbkeJC4ZdlAgq0dBG91IfOBoj6glIj7ktdqTbkPUSKiyx33nmeIr9je1O0bSH887gqQRyoJ03rUJNhWPe-%7ExdljrOqsuh51MzdBws8Sr1SqjDPpeZ1rBaf1PF50fRgveiMZw%7Ezdnu756sn4xugy9QPqw5YlDd3IqX3J9EMvaSqEgM0bkzeak3KTofHGEGsR9GFT9JaUxyfN10XwgVjwl2DEijd49iFNXWrwB1t-NgJia7x9wKE1tQsHUfqvE8m-WOl5lwTBKHQ6TMB1eGoFYqLqJ7rBxAjZynqHbyi6mHwA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.35.7.99, 13.35.7.14, 13.35.7.93, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.35.7.99|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4212859520 (3.9G) [application/octet-stream]\n",
            "Saving to: ‘llama-2-7b-chat.ggmlv3.q4_1.bin’\n",
            "\n",
            "llama-2-7b-chat.ggm 100%[===================>]   3.92G  64.1MB/s    in 38s     \n",
            "\n",
            "2023-11-13 18:03:47 (104 MB/s) - ‘llama-2-7b-chat.ggmlv3.q4_1.bin’ saved [4212859520/4212859520]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installing LangChain and Llama.cpp with `pip` (if not yet installed)\n",
        "!pip install langchain llama-cpp-python==0.1.78"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "l9Z1nYpfqFHi",
        "outputId": "379c4635-27df-43e4-99d7-690e38fe03b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.335)\n",
            "Requirement already satisfied: llama-cpp-python==0.1.78 in /usr/local/lib/python3.10/dist-packages (0.1.78)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (5.6.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.63)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ri9eXyzGiPBN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Setting up Llama 2\n",
        "#@markdown Specify the path to your model checkpoint in `model_path`.<br>If you downloaded Llama in this notebook, use the default path: `./llama-2-7b-chat.ggmlv3.q4_1.bin`\n",
        "model_path = \"./llama-2-7b-chat.ggmlv3.q4_1.bin\" #@param {type:\"string\"}\n",
        "#@markdown Choose the `description` of the dataset used to pretrain your StyleGAN `model` of choice.\n",
        "description = \"human face\" #@param [\"human face\", \"car\", \"painting\"]\n",
        "use_llm = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initializing Llama 2\n",
        "\n",
        "from langchain.llms import LlamaCpp\n",
        "#from langchain.chains import LLMChain\n",
        "#from langchain.callbacks.manager import CallbackManager\n",
        "#from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "# Callbacks support token-wise streaming\n",
        "#callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "start = time.time()\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    temperature=0.75,\n",
        "    max_tokens=2000,\n",
        "    top_p=1#,\n",
        "    #callback_manager=callback_manager,\n",
        "    #verbose=True,  # Verbose is required to pass to the callback manager\n",
        ")\n",
        "print(f\"Initialization took {time.time()-start} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "PmmJThEhpBt3",
        "outputId": "ce4b544e-6671-4bae-ba21-c218cec7550e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialization took 0.3231534957885742 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Refining text prompt with Llama 2\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Context: Modify the text in the following (text, class) tuple to be a suitable description relevant to the class. State your final answer only, without explanation.\n",
        "Examples:\n",
        "    (a happy sky, face) => a happy girl\n",
        "    (a sad face, face) => a sad face\n",
        "    (a colorful boy, face) => a cheerful boy\n",
        "Question: ({text_prompt}, {description}) =>\n",
        "\"\"\"\n",
        "\n",
        "start = time.time()\n",
        "text_prompt_ = llm(prompt).split(':')[-1].strip()\n",
        "print(f\"Refinement took {time.time()-start} seconds.\")\n",
        "print(f\"The refined text prompt is '{text_prompt_}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "fWciKeO1vslK",
        "outputId": "e89260ca-0334-454b-ea44-d7990aa1ac8b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refinement took 82.65573573112488 seconds.\n",
            "The refined text prompt is 'a grumpy man'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 1: Latent Optimization"
      ],
      "metadata": {
        "id": "JZ06F1xJ4un7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_llm:\n",
        "    text_prompt = text_prompt_"
      ],
      "metadata": {
        "id": "jDiKbnZ55EuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 2: Latent Mapper"
      ],
      "metadata": {
        "id": "UaXa6U9c4y-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_llm:\n",
        "    text_prompt = text_prompt_"
      ],
      "metadata": {
        "id": "Ek_IOiyO5FNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 3: Global Direction"
      ],
      "metadata": {
        "id": "IwJGh6U041rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_llm:\n",
        "    text_prompt = text_prompt_"
      ],
      "metadata": {
        "id": "4UdnD0IF5FpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "https://dev.to/eteimz/running-large-language-models-on-the-cpu-3a46  \n",
        "https://python.langchain.com/docs/integrations/llms/llamacpp  "
      ],
      "metadata": {
        "id": "_ZNDhREl4YJ4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}