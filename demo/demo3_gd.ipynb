{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP4S0NMlCr80E3L/wu3Xr00"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# StyleCLIP+: Towards an Extended and User-Friendly StyleCLIP, <b>Gradio Demo 3</b>\n","Fall 2023 CS470 Introduction to Artificial Intelligence Project 2 (Implementation), Team 3  \n","Myeongseok Kwon<sup>1</sup>, Junhak Ha<sup>2</sup>, Dongwan Hong<sup>3</sup>, Kyeongmin Lee<sup>4</sup>, Bryan Nathanael Wijaya<sup>5*</sup>  \n","<small><sup>1</sup>20170042, <sup>2</sup>20190683, <sup>3</sup>20190696, <sup>4</sup>20200429, <sup>5</sup>20200735, <sup>*</sup>Team Leader</small>\n","\n","## Simplified Global Direction"],"metadata":{"id":"uGhCRPvlpkOt"}},{"cell_type":"code","source":["#@title Setting up Gradio for demo\n","!pip install kaleido cohere openai tiktoken gradio -q\n","import gradio as gr"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"i_DS_WfFpnPG","executionInfo":{"status":"ok","timestamp":1701539511345,"user_tz":-540,"elapsed":34815,"user":{"displayName":"Bryan Nathanael Wijaya","userId":"03815891222670931187"}},"outputId":"7d24597a-b648-43a8-bb1b-f5ddeb1d40f1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["#@title Environment setup\n","\n","import os\n","# General environment setup for global directions\n","os.chdir('/content')\n","CODE_DIR = 'encoder4editing'\n","\n","! pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","!git clone https://github.com/omertov/encoder4editing.git $CODE_DIR\n","!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n","!sudo unzip ninja-linux.zip -d /usr/local/bin/\n","!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n","\n","%cd\n","%cd /content/\n","\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","!git clone https://github.com/bramyeon/StyleCLIP\n","\n","os.chdir(f'./{CODE_DIR}')\n","\n","from argparse import Namespace\n","import time\n","import os\n","import sys\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision.transforms as transforms\n","\n","sys.path.append(\".\")\n","sys.path.append(\"..\")\n","\n","from utils.common import tensor2im\n","from models.psp import pSp\n","%load_ext autoreload\n","%autoreload 2\n","%matplotlib inline\n","\n","%cd\n","%cd /content/StyleCLIP/global_torch/\n","\n","dataset_name = 'ffhq'\n","if not os.path.isfile('./model/'+dataset_name+'.pkl'):\n","        url='https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/'\n","        name='stylegan2-'+dataset_name+'-config-f.pkl'\n","        os.system('wget ' +url+name + '  -P  ./model/')\n","        os.system('mv ./model/'+name+' ./model/'+dataset_name+'.pkl')\n","# input prepare data\n","import clip\n","from manipulate import Manipulator\n","from StyleCLIP import GetDt,GetBoundary\n","from gdown import download as drive_download\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device,jit=False)\n","\n","network_pkl='./model/'+dataset_name+'.pkl'\n","device = torch.device('cuda')\n","M=Manipulator()\n","M.device=device\n","G=M.LoadModel(network_pkl,device)\n","M.G=G\n","M.SetGParameters()\n","num_img=100_000\n","M.GenerateS(num_img=num_img)\n","M.GetCodeMS()\n","np.set_printoptions(suppress=True)\n","\n","file_path='./npy/'+dataset_name+'/'\n","fs3=np.load(file_path+'fs3.npy')\n","\n","drive_download(\"https://drive.google.com/uc?id=1O8OLrVNOItOJoNGMyQ8G8YRTeTYEfs0P\", \"/content/encoder4editing/e4e_ffhq_encode.pt\", quiet=False)\n","\n","os.chdir('/content/encoder4editing')\n","\n","EXPERIMENT_ARGS = {\n","        \"model_path\": \"e4e_ffhq_encode.pt\"\n","    }\n","EXPERIMENT_ARGS['transform'] = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n","resize_dims = (256, 256)\n","\n","model_path = EXPERIMENT_ARGS['model_path']\n","ckpt = torch.load(model_path, map_location='cpu')\n","opts = ckpt['opts']\n","# pprint.pprint(opts)  # Display full options used\n","# update the training options\n","opts['checkpoint_path'] = model_path\n","opts= Namespace(**opts)\n","net = pSp(opts)\n","net.eval()\n","net.cuda()\n","from PIL import Image"],"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"s06nLIqbptGu","executionInfo":{"status":"ok","timestamp":1701539700197,"user_tz":-540,"elapsed":188855,"user":{"displayName":"Bryan Nathanael Wijaya","userId":"03815891222670931187"}},"outputId":"b7387794-d367-46ae-b7a3-128cb6f69be9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.7.1+cu110 (from versions: 1.11.0, 1.11.0+cpu, 1.11.0+cu102, 1.11.0+cu113, 1.11.0+cu115, 1.11.0+rocm4.3.1, 1.11.0+rocm4.5.2, 1.12.0, 1.12.0+cpu, 1.12.0+cu102, 1.12.0+cu113, 1.12.0+cu116, 1.12.0+rocm5.0, 1.12.0+rocm5.1.1, 1.12.1, 1.12.1+cpu, 1.12.1+cu102, 1.12.1+cu113, 1.12.1+cu116, 1.12.1+rocm5.0, 1.12.1+rocm5.1.1, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.0+cu117.with.pypi.cudnn, 1.13.0+rocm5.1.1, 1.13.0+rocm5.2, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 1.13.1+cu117.with.pypi.cudnn, 1.13.1+rocm5.1.1, 1.13.1+rocm5.2, 2.0.0, 2.0.0+cpu, 2.0.0+cpu.cxx11.abi, 2.0.0+cu117, 2.0.0+cu117.with.pypi.cudnn, 2.0.0+cu118, 2.0.0+rocm5.3, 2.0.0+rocm5.4.2, 2.0.1, 2.0.1+cpu, 2.0.1+cpu.cxx11.abi, 2.0.1+cu117, 2.0.1+cu117.with.pypi.cudnn, 2.0.1+cu118, 2.0.1+rocm5.3, 2.0.1+rocm5.4.2, 2.1.0, 2.1.0+cpu, 2.1.0+cpu.cxx11.abi, 2.1.0+cu118, 2.1.0+cu121, 2.1.0+cu121.with.pypi.cudnn, 2.1.0+rocm5.5, 2.1.0+rocm5.6, 2.1.1, 2.1.1+cpu, 2.1.1+cpu.cxx11.abi, 2.1.1+cu118, 2.1.1+cu121, 2.1.1+cu121.with.pypi.cudnn, 2.1.1+rocm5.5, 2.1.1+rocm5.6)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.7.1+cu110\u001b[0m\u001b[31m\n","\u001b[0mCloning into 'encoder4editing'...\n","remote: Enumerating objects: 172, done.\u001b[K\n","remote: Counting objects: 100% (172/172), done.\u001b[K\n","remote: Compressing objects: 100% (116/116), done.\u001b[K\n","remote: Total 172 (delta 58), reused 136 (delta 52), pack-reused 0\u001b[K\n","Receiving objects: 100% (172/172), 33.43 MiB | 18.71 MiB/s, done.\n","Resolving deltas: 100% (58/58), done.\n","--2023-12-02 17:51:58--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n","Resolving github.com (github.com)... 140.82.112.3\n","Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231202T175158Z&X-Amz-Expires=300&X-Amz-Signature=0e04521e5260dde65df48c66a6280f7b33e31d7d342996ee6c030dbf4bec6bf4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n","--2023-12-02 17:51:58--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231202T175158Z&X-Amz-Expires=300&X-Amz-Signature=0e04521e5260dde65df48c66a6280f7b33e31d7d342996ee6c030dbf4bec6bf4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 77854 (76K) [application/octet-stream]\n","Saving to: ‘ninja-linux.zip’\n","\n","ninja-linux.zip     100%[===================>]  76.03K  --.-KB/s    in 0.02s   \n","\n","2023-12-02 17:51:58 (3.39 MB/s) - ‘ninja-linux.zip’ saved [77854/77854]\n","\n","Archive:  ninja-linux.zip\n","  inflating: /usr/local/bin/ninja    \n","update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n","/root\n","/content\n","Collecting ftfy\n","  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m637.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.12)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.1.3\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-urkxq1ed\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-urkxq1ed\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.8.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=ae53f7721d79714543112a63603ef6fad62e7de57a1a481caabf1bc5eb4b4910\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2f48acpo/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n","Cloning into 'StyleCLIP'...\n","remote: Enumerating objects: 925, done.\u001b[K\n","remote: Counting objects: 100% (307/307), done.\u001b[K\n","remote: Compressing objects: 100% (159/159), done.\u001b[K\n","remote: Total 925 (delta 186), reused 211 (delta 142), pack-reused 618\u001b[K\n","Receiving objects: 100% (925/925), 200.06 MiB | 42.89 MiB/s, done.\n","Resolving deltas: 100% (331/331), done.\n","/root\n","/content/StyleCLIP/global_torch\n"]},{"output_type":"stream","name":"stderr","text":["100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 166MiB/s]\n"]},{"output_type":"stream","name":"stdout","text":["name:conv1_resolution_4 Resolution: 4, InC: 512, OutC:512, w_dim: 512\n","name:toRGB_resolution_4 InC: 512, OutC:3, w_dim: 512\n","name:conv0_resolution_8 Resolution: 8, InC: 512, OutC:512, w_dim: 512\n","name:conv1_resolution_8 Resolution: 8, InC: 512, OutC:512, w_dim: 512\n","name:toRGB_resolution_8 InC: 512, OutC:3, w_dim: 512\n","name:conv0_resolution_16 Resolution: 16, InC: 512, OutC:512, w_dim: 512\n","name:conv1_resolution_16 Resolution: 16, InC: 512, OutC:512, w_dim: 512\n","name:toRGB_resolution_16 InC: 512, OutC:3, w_dim: 512\n","name:conv0_resolution_32 Resolution: 32, InC: 512, OutC:512, w_dim: 512\n","name:conv1_resolution_32 Resolution: 32, InC: 512, OutC:512, w_dim: 512\n","name:toRGB_resolution_32 InC: 512, OutC:3, w_dim: 512\n","name:conv0_resolution_64 Resolution: 64, InC: 512, OutC:512, w_dim: 512\n","name:conv1_resolution_64 Resolution: 64, InC: 512, OutC:512, w_dim: 512\n","name:toRGB_resolution_64 InC: 512, OutC:3, w_dim: 512\n","name:conv0_resolution_128 Resolution: 128, InC: 512, OutC:256, w_dim: 512\n","name:conv1_resolution_128 Resolution: 128, InC: 256, OutC:256, w_dim: 512\n","name:toRGB_resolution_128 InC: 256, OutC:3, w_dim: 512\n","name:conv0_resolution_256 Resolution: 256, InC: 256, OutC:128, w_dim: 512\n","name:conv1_resolution_256 Resolution: 256, InC: 128, OutC:128, w_dim: 512\n","name:toRGB_resolution_256 InC: 128, OutC:3, w_dim: 512\n","name:conv0_resolution_512 Resolution: 512, InC: 128, OutC:64, w_dim: 512\n","name:conv1_resolution_512 Resolution: 512, InC: 64, OutC:64, w_dim: 512\n","name:toRGB_resolution_512 InC: 64, OutC:3, w_dim: 512\n","name:conv0_resolution_1024 Resolution: 1024, InC: 64, OutC:32, w_dim: 512\n","name:conv1_resolution_1024 Resolution: 1024, InC: 32, OutC:32, w_dim: 512\n","name:toRGB_resolution_1024 InC: 32, OutC:3, w_dim: 512\n","name:conv1_resolution_4 Resolution: 4, InC: 512, OutC:512, w_dim: 512\n","name:toRGB_resolution_4 InC: 512, OutC:3, w_dim: 512\n","name:conv0_resolution_8 Resolution: 8, InC: 512, OutC:512, w_dim: 512\n","name:conv1_resolution_8 Resolution: 8, InC: 512, OutC:512, w_dim: 512\n","name:toRGB_resolution_8 InC: 512, OutC:3, w_dim: 512\n","name:conv0_resolution_16 Resolution: 16, InC: 512, OutC:512, w_dim: 512\n","name:conv1_resolution_16 Resolution: 16, InC: 512, OutC:512, w_dim: 512\n","name:toRGB_resolution_16 InC: 512, OutC:3, w_dim: 512\n","name:conv0_resolution_32 Resolution: 32, InC: 512, OutC:512, w_dim: 512\n","name:conv1_resolution_32 Resolution: 32, InC: 512, OutC:512, w_dim: 512\n","name:toRGB_resolution_32 InC: 512, OutC:3, w_dim: 512\n","name:conv0_resolution_64 Resolution: 64, InC: 512, OutC:512, w_dim: 512\n","name:conv1_resolution_64 Resolution: 64, InC: 512, OutC:512, w_dim: 512\n","name:toRGB_resolution_64 InC: 512, OutC:3, w_dim: 512\n","name:conv0_resolution_128 Resolution: 128, InC: 512, OutC:256, w_dim: 512\n","name:conv1_resolution_128 Resolution: 128, InC: 256, OutC:256, w_dim: 512\n","name:toRGB_resolution_128 InC: 256, OutC:3, w_dim: 512\n","name:conv0_resolution_256 Resolution: 256, InC: 256, OutC:128, w_dim: 512\n","name:conv1_resolution_256 Resolution: 256, InC: 128, OutC:128, w_dim: 512\n","name:toRGB_resolution_256 InC: 128, OutC:3, w_dim: 512\n","name:conv0_resolution_512 Resolution: 512, InC: 128, OutC:64, w_dim: 512\n","name:conv1_resolution_512 Resolution: 512, InC: 64, OutC:64, w_dim: 512\n","name:toRGB_resolution_512 InC: 64, OutC:3, w_dim: 512\n","name:conv0_resolution_1024 Resolution: 1024, InC: 64, OutC:32, w_dim: 512\n","name:conv1_resolution_1024 Resolution: 1024, InC: 32, OutC:32, w_dim: 512\n","name:toRGB_resolution_1024 InC: 32, OutC:3, w_dim: 512\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1O8OLrVNOItOJoNGMyQ8G8YRTeTYEfs0P\n","To: /content/encoder4editing/e4e_ffhq_encode.pt\n","100%|██████████| 1.20G/1.20G [00:15<00:00, 76.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loading e4e over the pSp framework from checkpoint: e4e_ffhq_encode.pt\n"]}]},{"cell_type":"code","source":["#@title Demo 3 Gradio\n","\n","def demo3_fn(dataset_name, original_image, target, neutral, alpha, beta):\n","    # Align image\n","    if dataset_name == 'ffhq':\n","        experiment_type = 'ffhq_encode'\n","    else:\n","        experiment_type = 'others'\n","\n","    original_image = Image.fromarray(original_image)\n","    original_image = original_image.convert('RGB')\n","    image_path = 'original_image.jpg'\n","    original_image.save(image_path)\n","\n","    if experiment_type == \"ffhq_encode\" and 'shape_predictor_68_face_landmarks.dat' not in os.listdir():\n","        !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n","        !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n","\n","    def run_alignment(image_path):\n","        import dlib\n","        from utils.alignment import align_face\n","        predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n","        aligned_image = align_face(filepath=image_path, predictor=predictor)\n","        return aligned_image\n","\n","    if experiment_type == \"ffhq_encode\":\n","        input_image = run_alignment(image_path)\n","    else:\n","        input_image = original_image\n","\n","    input_image.resize(resize_dims)\n","\n","    # Invert the image\n","    img_transforms = EXPERIMENT_ARGS['transform']\n","    transformed_image = img_transforms(input_image)\n","\n","    def display_alongside_source_image(result_image, source_image):\n","        res = np.concatenate([np.array(source_image.resize(resize_dims)),\n","                            np.array(result_image.resize(resize_dims))], axis=1)\n","        return Image.fromarray(res)\n","\n","    def run_on_batch(inputs, net):\n","        images, latents = net(inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True)\n","        if experiment_type == 'cars_encode':\n","            images = images[:, :, 32:224, :]\n","        return images, latents\n","\n","    with torch.no_grad():\n","        images, latents = run_on_batch(transformed_image.unsqueeze(0), net)\n","        result_image, latent = images[0], latents[0]\n","    torch.save(latents, 'latents.pt')\n","\n","    mode='real image'\n","\n","    if mode == 'real image':\n","        img_index = 0\n","        latents=torch.load('/content/encoder4editing/latents.pt')\n","        dlatents_loaded=M.G.synthesis.W2S(latents)\n","\n","        img_indexs=[img_index]\n","        dlatents_loaded=M.S2List(dlatents_loaded)\n","\n","        dlatent_tmp=[tmp[img_indexs] for tmp in dlatents_loaded]\n","    elif mode == 'generated image':\n","        img_indexs=[img_index]\n","        dlatents_loaded=M.S2List(dlatents_loaded)\n","        dlatent_tmp=[tmp[img_indexs] for tmp in M.dlatents]\n","    M.num_images=len(img_indexs)\n","\n","    M.alpha=[0]\n","    M.manipulate_layers=[0]\n","    codes,out=M.EditOneC(0,dlatent_tmp)\n","    original=Image.fromarray(out[0,0]).resize((512,512))\n","    M.manipulate_layers=None\n","\n","    classnames=[target,neutral]\n","    dt=GetDt(classnames,model)\n","\n","    # make the original and generated images up-and-down\n","    def display(original, generated):\n","        min_width = min(original.width, generated.width)\n","        original = original.resize((min_width, int(original.height * min_width / original.width)))\n","        generated = generated.resize((min_width, int(generated.height * min_width / generated.width)))\n","        new_width = min_width\n","        new_height = original.height + generated.height\n","        new_image = Image.new('RGB', (new_width, new_height))\n","        new_image.paste(original, (0, 0))\n","        new_image.paste(generated, (0, original.height))\n","        return new_image\n","\n","    M.alpha=[alpha]\n","    boundary_tmp2,c=GetBoundary(fs3,dt,M,threshold=beta)\n","    codes=M.MSCode(dlatent_tmp,boundary_tmp2)\n","    out=M.GenerateImg(codes)\n","    generated=Image.fromarray(out[0,0])#.resize((512,512))\n","    result = display(original, generated)\n","\n","    return result\n","\n","demo3 = gr.Interface(\n","    fn=demo3_fn, title=\"CS470 Team 3 Global Direction Demo\",\n","    inputs=[gr.Dropdown(choices=['ffhq'], value='ffhq', label='Image domain (ffhq: face)'), \\\n","            gr.Image(label=\"Original image\"), \\\n","            gr.Textbox(lines=1, placeholder=\"e.g. a face with blue eyes\", label=\"Target prompt\"), \\\n","            gr.Textbox(lines=1, placeholder=\"e.g. a face with eyes\", label=\"Neutral prompt\"), \\\n","            gr.Slider(-10, 10, value=4, step=0.1, label='Manipulation strength (alpha)'), \\\n","            gr.Slider(0.08, 0.3, value=0.15, step=0.01,label='Features intertwining degree (beta)')],\n","    outputs=[gr.Image(label=\"Original vs. target image\")])\n","\n","if __name__ == \"__main__\":\n","    demo3.launch(show_api=False, share=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":611},"id":"IiDYtSnAp4ip","executionInfo":{"status":"ok","timestamp":1701539724696,"user_tz":-540,"elapsed":4827,"user":{"displayName":"Bryan Nathanael Wijaya","userId":"03815891222670931187"}},"outputId":"8dc8c435-3be9-4498-d1fe-5008da83cb7b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://da26e6e9fa5e010473.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://da26e6e9fa5e010473.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"3X3VOkr4q6Xj"},"execution_count":null,"outputs":[]}]}