{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# StyleCLIP+: Towards an Extended and User-Friendly StyleCLIP, <b>Gradio Demo 1</b>\n","Fall 2023 CS470 Introduction to Artificial Intelligence Project 2 (Implementation), Team 3  \n","Myeongseok Kwon<sup>1</sup>, Junhak Ha<sup>2</sup>, Dongwan Hong<sup>3</sup>, Kyeongmin Lee<sup>4</sup>, Bryan Nathanael Wijaya<sup>5*</sup>  \n","<small><sup>1</sup>20170042, <sup>2</sup>20190683, <sup>3</sup>20190696, <sup>4</sup>20200429, <sup>5</sup>20200735, <sup>*</sup>Team Leader</small>\n","\n","## Ablation Study of LLM Usage for Text Prompt Refinement"],"metadata":{"id":"ofF4wX1ojGWh"}},{"cell_type":"code","source":["#@title Setting up Gradio for demo\n","!pip install kaleido cohere openai tiktoken gradio -q\n","import gradio as gr"],"metadata":{"id":"RsbaFs_SGYaC","executionInfo":{"status":"ok","timestamp":1701542420315,"user_tz":-540,"elapsed":35647,"user":{"displayName":"Bryan Nathanael Wijaya","userId":"03815891222670931187"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d19e1b2e-d921-4eae-dd8c-d2fc816c44e5","cellView":"form"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["#@title Environment setup\n","import time, os\n","import torch\n","from torchvision.utils import make_grid\n","from torchvision.transforms import ToPILImage\n","if not os.path.exists('./StyleCLIP'):\n","    !git clone https://github.com/bramyeon/StyleCLIP.git\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","from tqdm import tqdm\n","\n","# Development environment setup for latent optimization\n","os.chdir(f'./StyleCLIP')\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","# downloads StyleGAN's weights and facial recognition network weights\n","from StyleCLIP.utils import google_drive_paths, llm_setup, prompt_refiner\n","ckpt = 'stylegan2-ffhq-config-f.pt'\n","ckpt_id = google_drive_paths[ckpt].split('=')[-1]\n","ids = [ckpt_id, '1N0MZSqPRJpLfP4mFQCS14ikrVSe8vQlL']\n","for file_id in ids:\n","    downloaded = drive.CreateFile({'id':file_id})\n","    downloaded.FetchMetadata(fetch_all=True)\n","    downloaded.GetContentFile(downloaded.metadata['title'])\n","llm = llm_setup()\n","from StyleCLIP.optimization.run_optimization import main\n","from argparse import Namespace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plgfswXN9jOi","executionInfo":{"status":"ok","timestamp":1701542598543,"user_tz":-540,"elapsed":178231,"user":{"displayName":"Bryan Nathanael Wijaya","userId":"03815891222670931187"}},"outputId":"79fb99cc-979b-4e14-8e22-be485d91247f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'StyleCLIP'...\n","remote: Enumerating objects: 925, done.\u001b[K\n","remote: Counting objects: 100% (311/311), done.\u001b[K\n","remote: Compressing objects: 100% (168/168), done.\u001b[K\n","remote: Total 925 (delta 188), reused 206 (delta 137), pack-reused 614\u001b[K\n","Receiving objects: 100% (925/925), 200.06 MiB | 24.65 MiB/s, done.\n","Resolving deltas: 100% (332/332), done.\n","Collecting ftfy\n","  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.12)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.1.3\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-qn1igzf4\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-qn1igzf4\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.8.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=41675f6a13b625f6426988b43ef540d707566a10c3461bc08d2b46c72839d58a\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-68x_6wck/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n"]},{"output_type":"stream","name":"stderr","text":["AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"]}]},{"cell_type":"code","source":["#@title Demo 1 Gradio\n","\n","def demo1_fn(seed, text_prompt, optimization_steps, l2_lambda, id_lambda):\n","    model = \"StyleGAN2, FFHQ, human face, stylegan2-ffhq-config-f.pt\"\n","    architecture, dataset, description, ckpt = model.split(', ')\n","\n","    # setting up the arguments for Latent Optimization\n","    experiment_type = 'edit'\n","    latent_path = None\n","    stylespace = False\n","    create_video = False\n","\n","    args = {\n","        \"description\": text_prompt,\n","        \"ckpt\": ckpt,\n","        \"stylegan_size\": 1024,\n","        \"lr_rampup\": 0.05,\n","        \"lr\": 0.1,\n","        \"step\": optimization_steps,\n","        \"mode\": experiment_type,\n","        \"l2_lambda\": l2_lambda,\n","        \"id_lambda\": id_lambda,\n","        \"mse_lambda\": 0,\n","        'work_in_stylespace': stylespace,\n","        \"latent_path\": latent_path,\n","        \"truncation\": 0.7,\n","        \"save_intermediate_image_every\": 1 if create_video else 20,\n","        \"results_dir\": \"results\",\n","        \"ir_se50_weights\": \"model_ir_se50.pth\"\n","    }\n","\n","    torch.manual_seed(seed)\n","    result = main(Namespace(**args))\n","    result_image = ToPILImage()(make_grid(result.detach().cpu(), normalize=True, scale_each=True, padding=0)) #range=(-1, 1)\n","    h, w = result_image.size\n","    result_image.resize((h // 2, w // 2))\n","\n","    # text prompt refined version\n","    text_prompt = prompt_refiner(llm, text_prompt, description)\n","    args['description'] = text_prompt\n","    torch.manual_seed(seed)\n","    result_llm = main(Namespace(**args))\n","    result_image_llm = ToPILImage()(make_grid(result_llm.detach().cpu(), normalize=True, scale_each=True, padding=0)) #range=(-1, 1)\n","    h, w = result_image_llm.size\n","    result_image_llm.resize((h // 2, w // 2))\n","\n","    return result_image, args['description'], result_image_llm\n","\n","demo1 = gr.Interface(\n","    fn=demo1_fn, title=\"CS470 Team 3 LLM Demo\",\n","    inputs=[gr.Slider(0, 1000, step=1, value=470, label=\"Seed for original FACE image\"), \\\n","            gr.Textbox(lines=1, placeholder=\"e.g., a surprised book\", label=\"Target prompt\"), \\\n","            gr.Slider(0, 500, step=1, value=40, label=\"Optimization steps\"), \\\n","            gr.Slider(0, 0.1, step=0.001, value=0.008, label=\"L2-loss strength\"), \\\n","            gr.Slider(0, 0.1, step=0.001, value=0.005, label=\"ID-loss strength\")],\n","    outputs=[gr.Image(label=\"Original vs. target image (w/o LLM refinement)\"), \\\n","             gr.Textbox(label=\"LLM-refined target prompt\"), \\\n","             gr.Image(label=\"Original vs. target image (with LLM refinement)\")])\n","\n","if __name__ == \"__main__\":\n","    demo1.launch(show_api=False, share=True)"],"metadata":{"id":"Ue8Fxc_aHx_1","colab":{"base_uri":"https://localhost:8080/","height":611},"outputId":"d9044ce7-206f-4db5-e750-d1c165f114bf","executionInfo":{"status":"ok","timestamp":1701542603210,"user_tz":-540,"elapsed":4671,"user":{"displayName":"Bryan Nathanael Wijaya","userId":"03815891222670931187"}},"cellView":"form"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://eeb2560007890d3a19.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://eeb2560007890d3a19.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}]}]}